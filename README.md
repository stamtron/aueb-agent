# Multi-Agent Parallel Research Architecture

An advanced, agentic AI system implementing the **Parallel Research** pattern using Google's Agent Development Kit (ADK) and local Ollama models.

## Executive Summary

This project provides a sophisticated multi-level AI system designed to handle complex research queries by leveraging a "Panel of Experts" approach. It uses a root orchestrator to manage conversations and delegates deep inquiries to 7 distinct, specialized worker agents running concurrently via local Ollama models. A final verifier agent then synthesizes these 7 perspectives—fact-checking against the web if necessary—into a single, comprehensive, and well-reasoned response.

**Origins & Inspiration**: This architecture is named the "AUEB Agent" in the code as it was originally developed and showcased during a lecture I gave on AI Agents at the **Athens University of Economics and Business (AUEB)** for their MSc in Data Science course. Inspired by Andrej Karpathy's "LLM Council" concept ([see his llm-council repository here](https://github.com/karpathy/llm-council)), the architecture uses Ollama to run robust open-source models completely locally. This allows students to experiment with advanced multi-agent workflows and parallel generation techniques without the barrier of paying for expensive API keys.## Architecture

This project implements a multi-level agent architecture designed for deep research and verification:

1.  **Root Orchestrator**:
    *   The entry point for all user interactions.
    *   Handles casual conversation (greetings, chit-chat) directly.
    *   Delegates complex inquiries to the **Research Team**.

2.  **Research Team (Parallel System)**:
    *   **7 Parallel Workers**: A diverse panel of expert models (Logic, Coding, Creative, Vision, General) that analyze the query concurrently, without influencing each other.
    *   **Models**: The system is configured by default to use the following specialized local models (via Ollama):
        *   `llama3.2:latest` (General)
        *   `deepseek-coder:1.3b` (Reasoning)
        *   `qwen2.5-coder:7b` (Coding)
        *   `llava:latest` (Multimodal)
        *   `gemma2:2b` (Creative)
        *   `qwen2.5:3b` (Bilingual/Academic)
        *   `mistral:latest` (Large Scale Synthesis)
        
        *Note: These specific LLMs are provided merely as example baselines for the sake of this exercise. Because the agent communicates via standard interfaces, any of these models can and should be hot-swapped for newer, state-of-the-art open-source releases as they become available on Ollama.*

3.  **Verifier Agent**:
    *   Functions as the final synthesis layer. It takes the 7 distinct responses generated by the parallel workers and merges them into a single, cohesive, and comprehensive report.
    *   Acts as a safeguard against hallucinations: if the panel of experts disagree on a factual point, the Verifier uses its **DuckDuckGo Search** tool to actively fact-check the conflicting data against the live internet before returning the final result to the user.

## Prerequisites

*   **Python 3.10+**
*   **[uv](https://docs.astral.sh/uv/)**: Fast Python package manager.
*   **[Ollama](https://ollama.com/)**: Running locally/remotely to serve the LLMs.

## Setup

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/your-username/aueb-agent.git
    cd aueb-agent
    ```

2.  **Install dependencies**:
    ```bash
    make install
    ```

3.  **Configure Ollama Models**:
    Since the architecture asks for specific (and large) model IDs, we assume you have models mapped to these names. Run the provided helper script to alias your existing local models (e.g., `llama3`, `mistral`) to the architecture's required names:
    ```bash
    chmod +x setup_ollama_models.sh
    ./setup_ollama_models.sh
    ```
    *(Note: Edit this script first if you want to map different source models)*

## Running the Agent

### Web Interface (Playground)
Launch the full ADK Web UI to chat with the Root Orchestrator:
```bash
make playground
```
*   Access at: `http://localhost:8801` (Port changed to 8801)
*   Say "Hello" to test the Orchestrator.
*   Ask "What is the importance of attention in LLMs?" to trigger the full parallel research team.

### CLI Verification Script
To test the pipeline programmatically without the UI:
```bash
uv run verify_parallel.py
```

## Key Features & Implementations

*   **Ollama Compatibility Layer**: Includes a custom `OllamaLiteLlm` wrapper (`app/ollama_fix.py`) that transparently fixes JSON payload compatibility issues between LiteLLM and current Ollama API versions.
*   **Parallel Execution**: Uses `ParallelAgent` to run multiple LLM calls concurrently, significantly speeding up the "Panel of Experts" approach.
*   **Tool Integration**: Wraps the entire multi-agent system as a single tool (`parallel_verifier_system`) callable by the Orchestrator.

## Project Structure

```
aueb-agent/
├── app/
│   ├── agent.py            # Root Orchestrator definition
│   ├── agent_parallel.py   # Parallel Workers & Verifier system
│   ├── ollama_fix.py       # Compatibility patch for LiteLLM/Ollama
│   └── ...
├── verify_parallel.py      # CLI script to test the full flow
├── setup_ollama_models.sh  # Helper to alias local Ollama models
├── Makefile                # Command shortcuts
└── README.md               # This file
```
